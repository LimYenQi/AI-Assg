{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d02a6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7836c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_threshold = 0.3      #Non-Maximal Suppression - threshold for separating overlapping predictions\n",
    "min_confidence = 0.2     #hreshold for confidence score returned by the model which a prediction is considered true\n",
    "videoPath = \"video_1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca946ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile():\n",
    "    filetypes=(\n",
    "        (\"mp4 files\", \"*.mp4\"), \n",
    "        (\"avi files\", \"*.avi\")\n",
    "    )\n",
    "    \n",
    "    root.filename = filedialog.askopenfilename(\n",
    "        initialdir=\"/Users/Lim Yen Qi/Documents/Degree Y2S3/Artificial Intelligence/Assg/Code\",\n",
    "        title=\"Select a video\", \n",
    "        filetypes=filetypes\n",
    "    )\n",
    "    lblVideoPath.config(text=root.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8163ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    try: \n",
    "        num = float(entConfSc.get())\n",
    "        if 0.2 <= num < 1.0:\n",
    "            min_confidence = num\n",
    "            lblMsg.config(text=\" \")\n",
    "            #check path\n",
    "            if lblVideoPath[\"text\"] != \"\": \n",
    "                start_detection(nms_threshold, min_confidence, lblVideoPath[\"text\"])\n",
    "            else: \n",
    "                lblVideoPath.config(text=\"Please choose a file\")\n",
    "        else: \n",
    "            lblMsg.config(text=\"The number entered must in between 0.2 - 1.0 \\n(1.0 is not included)\")\n",
    "    except: \n",
    "        lblMsg.config(text=\"Invalid input. Please enter a number between 0.2 - 1.0 \\n(1.0 is not included)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a555fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pedestrian_detection_YOLO(image, model, layer_name, nms_threshold, min_confidence, personidz=0):\n",
    "    (H, W) = image.shape[:2]   #dimensions of the frame passed \n",
    "    results = []\n",
    "    \n",
    "    #blob = Binary Large OBject; a data type that can store binary data\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    model.setInput(blob)\n",
    "    layerOutputs = model.forward(layer_name)\n",
    "    #yolo model perform forward pass & will return the bounding box for the detections & the confidence value\n",
    "    #thus, layerOutputs is a list of output lists(bounding box, confidence value) \n",
    "    \n",
    "    boxes = []\n",
    "    centroids = []\n",
    "    confidences = []\n",
    "    \n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "            \n",
    "            #only get the detection for PERSON class (person class id = 0) & only if the confidence > min_confidence \n",
    "            if classID == personidz and confidence > min_confidence:\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                \n",
    "                #get the top-right coordinate of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                \n",
    "                #add them to the lists created\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                centroids.append((centerX, centerY))\n",
    "                confidences.append(float(confidence))\n",
    "                \n",
    "    # apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    delOverlap = cv2.dnn.NMSBoxes(boxes, confidences, min_confidence, nms_threshold)\n",
    "    \n",
    "    #check if there is any detection exists\n",
    "    if len(delOverlap) > 0:\n",
    "        #loop through all the detections \n",
    "        for i in delOverlap.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            # update the results list to consist of the person prediction probability, bounding box coordinates and the centroid\n",
    "            res = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "            results.append(res)\n",
    "            \n",
    "    # return the list of results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb1a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_detection(nms_threshold, min_confidence, videoPath):\n",
    "    labelsPath = \"coco.names\"\n",
    "    LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "    weights_path = \"yolov4-tiny.weights\"\n",
    "    config_path = \"yolov4-tiny.cfg\"\n",
    "    \n",
    "    model = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "    \n",
    "    #run OpenCV on GPU\n",
    "    model.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    model.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "    \n",
    "    layer_name = model.getLayerNames()\n",
    "    layer_name = [layer_name[i - 1] for i in model.getUnconnectedOutLayers()]\n",
    "\n",
    "    #videoPath = \"video_1.mp4\"\n",
    "    #pass 0 if want to use webcam\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "    \n",
    "    \n",
    "    #read all the frames from the video, break the loop if the video/frame ends\n",
    "    while True:\n",
    "        (grabbed, image) = cap.read()   #grabbed - A boolean indicating if the frame was successfully read or not.\n",
    "\n",
    "        if not grabbed:\n",
    "            break\n",
    "        image = imutils.resize(image, width=700)  #resize the image without changing the ratio\n",
    "        results = pedestrian_detection_YOLO(image, model, layer_name, nms_threshold, min_confidence, personidz=LABELS.index(\"person\"))\n",
    "\n",
    "        for res in results:\n",
    "            #draw the bounding boxes\n",
    "            #(image, topLeft, bottomRight, colour, thickness)\n",
    "            cv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "            #put text on the boxes\n",
    "            #(image, text, org, font, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n",
    "            cv2.putText(image, str(round(res[0], 6)), (res[1][0],res[1][1]), cv2.FONT_HERSHEY_PLAIN, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Pedestrian Detection - YOLO\",image)   #display the image in a window\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:   #break when esc key is pressed\n",
    "            break\n",
    "\n",
    "    cap.release()             #release the capture \n",
    "    cv2.destroyAllWindows()   #close the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa198d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Tk() \n",
    "root.title('Pedestrian Detection - YOLO')\n",
    "root.geometry(\"500x350\") #width * height\n",
    "\n",
    "\n",
    "lblConfidenceSc = Label(root, text=\"Minimum Confidence Score: \\n(0.2 <= x < 1.0) \", justify=RIGHT)\n",
    "lblConfidenceSc.grid(row=0, column=0, padx=(30,10), pady=(20,0), sticky = 'e') \n",
    "\n",
    "entConfSc = Entry(root, borderwidth=5)\n",
    "entConfSc.grid(row=0, column=1, padx=(0,0), pady=(20,0), sticky = 'w')\n",
    "\n",
    "lblMsg = Label(root, text=\" \", justify=LEFT, wraplengt=300)\n",
    "lblMsg.grid(row=1, column=1, padx=(0,0), pady=(0,10), sticky = 'w')\n",
    "\n",
    "lblVideoFile = Label(root, text=\"Choose a video file: \")\n",
    "lblVideoFile.grid(row=2, column=0, padx=(30,10), pady=(20,0), sticky = 'e')\n",
    "\n",
    "btnGetFile = Button(root, text=\"Browse\", command=openFile)\n",
    "btnGetFile.grid(row=2, column=1, padx=(0,0), pady=(20,0), sticky = 'w')\n",
    "\n",
    "lblMsgFile = Label(root, text=\"File chosen: \", justify=RIGHT)\n",
    "lblMsgFile.grid(row=3, column=0, padx=(0,10), pady=(0,20), stick='e')\n",
    "\n",
    "lblVideoPath = Label(root, text=\"\", justify=LEFT, wraplengt=300)\n",
    "lblVideoPath.grid(row=3, column=1, padx=(0,0), pady=(0,20), stick='w')\n",
    "\n",
    "btnRun = Button(root, text=\"Run\", width=\"8\", command=run) #command=functionName\n",
    "btnRun.grid(row=4, column=0, padx=(30,0), pady=(30,10), sticky = 'e')\n",
    "\n",
    "# sticky = 'e' : Align to Right.\n",
    "# sticky = 'w': Align to Left.\n",
    "# sticky = 'n': Align to Top.\n",
    "# sticky = 's': Align to bottom.\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c5dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
